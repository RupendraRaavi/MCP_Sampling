# FastMCP LLM Sampling Demo

This project demonstrates the LLM Sampling feature in `fastmcp`, showcasing how a server can request LLM completions and how different clients can handle these requests.

## Overview

This project contains a `fastmcp` server that exposes a tool for generating welcome notes. It demonstrates two primary client-server patterns for LLM sampling:

1.  **Server-Side Fallback**: A "thin" client (`clientopenai.py`) calls the tool, relying on the server's built-in fallback handler to perform the LLM completion using the OpenAI API.
2.  **Client-Side Handling**: A "thick" client (`client.py`) calls the same tool but intercepts the sampling request to provide its own mock response, overriding the server's handler.

Additionally, it includes an example of how to integrate the MCP server as a tool within a Google ADK agent (`sampling_agent/agent.py`).

## File Structure

-   `welcome_server.py`: The `FastMCP` server. It exposes a `generate_welcome_note_options` tool and has a fallback sampling handler that calls the OpenAI API.
-   `clientopenai.py`: A "thin" client that relies on the server's OpenAI handler to generate real content.
-   `client.py`: A "thick" client with its own mock `sampling_handler` that overrides the server's logic.
-   `sampling_agent/agent.py`: A Google ADK agent that uses `welcome_server.py` as a tool.
-   `.env`: An environment file to store your API key.

## Prerequisites

-   Python 3.10+
-   An OpenAI API key.

## Setup

1.  **Set up the Python Virtual Environment:**
    From the project root directory, run:
    ```bash
    # On macOS/Linux
    python3 -m venv .venv
    source .venv/bin/activate

    # On Windows
    python -m venv .venv
    .venv\Scripts\activate
    ```

2.  **Install Dependencies:**
    Install the required Python packages.
    ```bash
    pip install fastmcp "fastmcp[openai]" python-dotenv google-adk
    ```

3.  **Configure your OpenAI API Key:**
    Create a file named `.env` in the root directory (`D:\MCP_ADK\MCP_Sampling`) and add your OpenAI API key to it.
    ```ini
    # .env
    OPENAI_API_KEY="sk-..."
    ```

## Running the Examples

Make sure your virtual environment is activated for all examples.

### Example 1: Server-Side Sampling (Thin Client)

This test uses `clientopenai.py` and demonstrates the server performing the LLM completion.

1.  Ensure your `.env` file contains a valid `OPENAI_API_KEY`.
2.  Run the script from your terminal:
    ```bash
    python clientopenai.py
    ```
**Expected Output:** The script will connect to the server (which is started automatically), and you will see three distinct welcome note options generated by OpenAI.

### Example 2: Client-Side Sampling (Thick Client)

This test uses `client.py` and demonstrates the client providing a mock response.

1.  Run the script from your terminal:
    ```bash
    python client.py
    ```
**Expected Output:** The script will print a single formatted line ending with `"-- (handled by client sampling_handler)"`. This confirms the client's local handler was used instead of the server's OpenAI handler.

### Example 3: Google ADK Agent Integration

This test runs the `welcome_server` as a tool within an ADK agent.

1.  Ensure your `.env` file contains a valid `OPENAI_API_KEY`.
2.  Run the ADK web server from your terminal:
    ```bash
    adk serve -a sampling_agent/agent.py
    ```
3.  Open the URL provided by the command (e.g., `http://127.0.0.1:8080`) in your web browser.
4.  In the chat interface, ask the agent to generate notes. For example:
    > "generate welcome notes for a new intern"

**Expected Output:** The agent will use its MCP tool to connect to the `welcome_server.py` and display the AI-generated welcome notes in the chat interface.
